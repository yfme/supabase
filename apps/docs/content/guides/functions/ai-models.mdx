---
id: 'function-ai-models'
title: '运行 AI 模型'
description: '如何在 Edge Functions 中运行 AI 模型。'
subtitle: '如何在 Edge Functions 中运行 AI 模型。'
tocVideo: 'w4Rr_1whU-U'
---

[Supabase Edge Runtime](https://github.com/supabase/edge-runtime) 内置了用于运行 AI 模型的 API。您可以使用此 API 在 Edge Functions 中生成嵌入向量、构建对话工作流和执行其他 AI 相关任务。

## 设置

启用此 API 不需要安装外部依赖项或包。

您可以通过以下方式创建新的推理会话：

```ts
const model = new Supabase.ai.Session('model-name')
```

<Admonition type="tip">

要获取 API 的类型提示和检查，您可以在文件顶部从 `functions-js` 导入类型：

```ts
import 'jsr:@supabase/functions-js/edge-runtime.d.ts'
```

</Admonition>

## 运行模型推理

一旦实例化会话，您可以使用输入调用它来执行推理。根据您运行的模型，可能需要提供不同的选项（下面将讨论）。

```ts
const output = await model.run(input, options)
```

## 如何生成文本嵌入向量

现在让我们看看如何使用 `Supabase.ai` API 编写 Edge Function 来生成文本嵌入向量。目前，`Supabase.ai` API 仅支持 [gte-small](https://huggingface.co/Supabase/gte-small) 模型。

<Admonition type="tip">

`gte-small` 模型专门处理英文文本，任何长文本都将被截断为最多 512 个标记。虽然您可以提供超过 512 个标记的输入，但截断可能会影响准确性。

</Admonition>

```ts
const model = new Supabase.ai.Session('gte-small')

Deno.serve(async (req: Request) => {
  const params = new URL(req.url).searchParams
  const input = params.get('input')
  const output = await model.run(input, { mean_pool: true, normalize: true })
  return new Response(JSON.stringify(output), {
    headers: {
      'Content-Type': 'application/json',
      Connection: 'keep-alive',
    },
  })
})
```

## 使用大型语言模型 (LLM)

通过 [Ollama](https://ollama.com/) 和 [Mozilla Llamafile](https://github.com/Mozilla-Ocho/llamafile) 支持更大模型的推理。在第一阶段，您可以使用自管理的 Ollama 或 [Llamafile 服务器](https://www.docker.com/blog/a-quick-guide-to-containerizing-llamafile-with-docker-for-ai-applications/)。我们正在逐步推出对托管解决方案的支持。要申请早期访问权限，请填写[此表单](https://forms.supabase.com/supabase.ai-llm-early-access)。

<video width="99%" muted playsInline controls={true}>
  <source
    src="https://xguihxuzqibwxjnimxev.supabase.co/storage/v1/object/public/videos/docs/guides/edge-functions-inference-2.mp4"
    type="video/mp4"
  />
</video>

### 本地运行

<Tabs
  scrollable
  size="large"
  type="underlined"
  defaultActiveId="ollama"
  queryGroup="platform"
>
  <TabPanel id="ollama" label="Ollama">

[安装 Ollama](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) 并拉取 Mistral 模型

```bash
ollama pull mistral
```

在本地运行 Ollama 服务器

```bash
ollama serve
```

设置一个名为 AI_INFERENCE_API_HOST 的函数密钥，指向 Ollama 服务器

```bash
echo "AI_INFERENCE_API_HOST=http://host.docker.internal:11434" >> supabase/functions/.env
```

创建一个包含以下代码的新函数

```bash
supabase functions new ollama-test
```

```ts supabase/functions/ollama-test/index.ts
import 'jsr:@supabase/functions-js/edge-runtime.d.ts'
const session = new Supabase.ai.Session('mistral')

Deno.serve(async (req: Request) => {
  const params = new URL(req.url).searchParams
  const prompt = params.get('prompt') ?? ''

  // 获取输出流
  const output = await session.run(prompt, { stream: true })

  const headers = new Headers({
    'Content-Type': 'text/event-stream',
    Connection: 'keep-alive',
  })

  // 创建流
  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder()

      try {
        for await (const chunk of output) {
          controller.enqueue(encoder.encode(chunk.response ?? ''))
        }
      } catch (err) {
        console.error('Stream error:', err)
      } finally {
        controller.close()
      }
    },
  })

  // 将流返回给用户
  return new Response(stream, {
    headers,
  })
})
```

启动函数服务

```bash
supabase functions serve --env-file supabase/functions/.env
```

执行函数

```bash
curl --get "http://localhost:54321/functions/v1/ollama-test" \
--data-urlencode "prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj" \
-H "Authorization: $ANON_KEY"
```

</TabPanel>
<TabPanel id="llamafile" label="Mozilla Llamafile">

按照 [Llamafile 快速入门](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#quickstart) 在您的设备上下载并运行 Llamafile。

由于 Llamafile 提供了与 OpenAI API 兼容的服务器，您可以使用 `@supabase/functions-js` 或官方 OpenAI Deno SDK 与其交互。

<Tabs
  scrollable
  size="large"
  type="underlined"
  defaultActiveId="supabase-functions-js"
  queryGroup="sdk"
>
  <TabPanel id="supabase-functions-js" label="Supabase Functions JS">

设置一个名为 `AI_INFERENCE_API_HOST` 的函数密钥，指向 Llamafile 服务器

```bash
echo "AI_INFERENCE_API_HOST=http://host.docker.internal:8080" >> supabase/functions/.env
```

创建一个包含以下代码的新函数

```bash
supabase functions new llamafile-test
```

<Admonition type="note">

注意，这里的模型参数没有任何效果！模型取决于当前正在运行的 Llamafile！

</Admonition>

```ts supabase/functions/llamafile-test/index.ts
import 'jsr:@supabase/functions-js/edge-runtime.d.ts'
const session = new Supabase.ai.Session('LLaMA_CPP')

Deno.serve(async (req: Request) => {
  const params = new URL(req.url).searchParams
  const prompt = params.get('prompt') ?? ''

  // 获取输出（非流式）
  const output = await session.run(
    {
      messages: [
        {
          role: 'system',
          content:
            '你是 LLAMAfile，一个 AI 助手。你的首要任务是通过帮助用户满足他们的请求来实现用户满意度。',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
    },
    {
      mode: 'openaicompatible', // 推理 API 主机的模式。(默认: 'ollama')
      stream: false,
    }
  )

  console.log('done')
  return Response.json(output)
})
```

</TabPanel>
<TabPanel id="openai" label="OpenAI Deno SDK">

设置以下函数密钥，将 OpenAI SDK 指向 Llamafile 服务器：

```bash
echo "OPENAI_BASE_URL=http://host.docker.internal:8080/v1" >> supabase/functions/.env
echo "OPENAI_BASE_URL=OPENAI_API_KEY=sk-XXXXXXXX" >> supabase/functions/.env
```

创建一个包含以下代码的新函数

```bash
supabase functions new llamafile-test
```

<Admonition type="note">

注意，这里的模型参数没有任何效果！模型取决于当前正在运行的 Llamafile！

</Admonition>

```ts supabase/functions/llamafile-test/index.ts
import OpenAI from 'https://deno.land/x/openai@v4.53.2/mod.ts'

Deno.serve(async (req) => {
  const client = new OpenAI()
  const { prompt } = await req.json()
  const stream = true

  const chatCompletion = await client.chat.completions.create({
    model: 'LLaMA_CPP',
    stream,
    messages: [
      {
        role: 'system',
        content:
          '你是 LLAMAfile，一个 AI 助手。你的首要任务是通过帮助用户满足他们的请求来实现用户满意度。',
      },
      {
        role: 'user',
        content: prompt,
      },
    ],
  })

  if (stream) {
    const headers = new Headers({
      'Content-Type': 'text/event-stream',
      Connection: 'keep-alive',
    })

    // 创建流
    const stream = new ReadableStream({
      async start(controller) {
        const encoder = new TextEncoder()

        try {
          for await (const part of chatCompletion) {
            controller.enqueue(encoder.encode(part.choices[0]?.delta?.content || ''))
          }
        } catch (err) {
          console.error('Stream error:', err)
        } finally {
          controller.close()
        }
      },
    })

    // 将流返回给用户
    return new Response(stream, {
      headers,
    })
  }

  return Response.json(chatCompletion)
})
```

</TabPanel>
</Tabs>

启动函数服务

```bash
supabase functions serve --env-file supabase/functions/.env
```

执行函数

```bash
curl --get "http://localhost:54321/functions/v1/llamafile-test" \
 --data-urlencode "prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj" \
 -H "Authorization: $ANON_KEY"
```

</TabPanel>
</Tabs>

### 部署到生产环境

一旦函数在本地正常工作，就可以部署到生产环境了。

部署 Ollama 或 Llamafile 服务器，并设置一个名为 `AI_INFERENCE_API_HOST` 的函数密钥，指向已部署的服务器

```bash
supabase secrets set AI_INFERENCE_API_HOST=https://path-to-your-llm-server/
```

部署 Supabase 函数

```bash
supabase functions deploy
```

执行函数

```bash
curl --get "https://project-ref.supabase.co/functions/v1/ollama-test" \
 --data-urlencode "prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj" \
 -H "Authorization: $ANON_KEY"
```

如上面视频所示，在本地运行 Ollama 通常比在具有专用 GPU 的服务器上运行要慢。我们正在与 Ollama 团队合作，以提高本地性能。

未来，Supabase 平台将提供托管 LLM API。Supabase 将为您扩展和管理 API 和 GPU。要申请早期访问权限，请填写[此表单](https://forms.supabase.com/supabase.ai-llm-early-access)。