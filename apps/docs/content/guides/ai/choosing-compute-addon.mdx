---
id: 'ai-choosing-compute-addon'
title: '选择您的计算附加组件'
description: '为您的向量工作负载选择合适的计算附加组件。'
subtitle: '为您的向量工作负载选择合适的计算附加组件。'
sidebar_label: '选择计算附加组件'
---

您有两个选项来扩展您的向量工作负载：

1. 增加数据库的大小。本指南将帮助您为工作负载选择合适的大小。
2. 将工作负载分散到多个数据库。有关此方法的更多详细信息，请参阅[规模化工程](engineering-for-scale)。

## 维度

嵌入的维度数量是选择合适计算附加组件的最重要因素。一般来说，维度越低，性能越好。我们为一些常见的嵌入维度提供了下面的指导。对于每个基准测试，我们使用 [Vecs](https://github.com/supabase/vecs) 创建集合，将嵌入上传到单个表，并为嵌入列创建 `IVFFlat` 和 `HNSW` 索引，用于 `inner-product` 距离测量。然后，我们运行一系列查询来测量不同计算附加组件的性能：

## HNSW

### 384 维 [#hnsw-384-dimensions]

此基准测试使用 dbpedia-entities-openai-1M 数据集，其中包含 1,000,000 个文本嵌入，为 384 维嵌入重新生成。每个嵌入使用 [gte-small](https://huggingface.co/Supabase/gte-small) 生成。

<Tabs
  scrollable
  size="small"
  type="underlined"
  defaultActiveId="gte384"
  queryGroup="benchmark"
>
<TabPanel id="gte384" label="gte-small-384">

| 计算大小 | 向量数   | m   | ef_construction | ef_search | QPS  | 延迟平均值 | 延迟 p95  | RAM 使用量  | RAM    |
| -------- | ------- | --- | --------------- | --------- | ---- | ---------- | --------- | ---------- | ------ |
| Micro    | 100,000 | 16  | 64              | 60        | 580  | 0.017 秒   | 0.024 秒  | 1.2 (交换) | 1 GB   |
| Small    | 250,000 | 24  | 64              | 60        | 440  | 0.022 秒   | 0.033 秒  | 2 GB       | 2 GB   |
| Medium   | 500,000 | 24  | 64              | 80        | 350  | 0.028 秒   | 0.045 秒  | 4 GB       | 4 GB   |
| Large    | 1,000,000| 32  | 80              | 100       | 270  | 0.073 秒   | 0.108 秒  | 7 GB       | 8 GB   |
| XL       | 1,000,000| 32  | 80              | 100       | 525  | 0.038 秒   | 0.059 秒  | 9 GB       | 16 GB  |
| 2XL      | 1,000,000| 32  | 80              | 100       | 790  | 0.025 秒   | 0.037 秒  | 9 GB       | 32 GB  |
| 4XL      | 1,000,000| 32  | 80              | 100       | 1650 | 0.015 秒   | 0.018 秒  | 11 GB      | 64 GB  |
| 8XL      | 1,000,000| 32  | 80              | 100       | 2690 | 0.015 秒   | 0.016 秒  | 13 GB      | 128 GB |
| 12XL     | 1,000,000| 32  | 80              | 100       | 3900 | 0.014 秒   | 0.016 秒  | 13 GB      | 192 GB |
| 16XL     | 1,000,000| 32  | 80              | 100       | 4200 | 0.014 秒   | 0.016 秒  | 20 GB      | 256 GB |

基准测试的准确率为 0.99。

</TabPanel>
</Tabs>

### 960 维 [#hnsw-960-dimensions]

此基准测试使用 [gist-960](http://corpus-texmex.irisa.fr/) 数据集，其中包含 1,000,000 个图像嵌入。每个嵌入为 960 维。

<Tabs
  scrollable
  size="small"
  type="underlined"
  defaultActiveId="gist960"
  queryGroup="benchmark"
>
<TabPanel id="gist960" label="gist-960">

| 计算大小 | 向量数   | m   | ef_construction | ef_search | QPS  | 延迟平均值 | 延迟 p95  | RAM 使用量    | RAM    |
| -------- | ------- | --- | --------------- | --------- | ---- | ---------- | --------- | ------------ | ------ |
| Micro    | 30,000  | 16  | 64              | 65        | 430  | 0.024 秒   | 0.034 秒  | 1.2 GB (交换) | 1 GB   |
| Small    | 100,000 | 32  | 80              | 60        | 260  | 0.040 秒   | 0.054 秒  | 2.2 GB (交换) | 2 GB   |
| Medium   | 250,000 | 32  | 80              | 90        | 120  | 0.083 秒   | 0.106 秒  | 4 GB         | 4 GB   |
| Large    | 500,000 | 32  | 80              | 120       | 160  | 0.063 秒   | 0.087 秒  | 7 GB         | 8 GB   |
| XL       | 1,000,000| 32  | 80              | 200       | 200  | 0.049 秒   | 0.072 秒  | 13 GB        | 16 GB  |
| 2XL      | 1,000,000| 32  | 80              | 200       | 340  | 0.025 秒   | 0.029 秒  | 17 GB        | 32 GB  |
| 4XL      | 1,000,000| 32  | 80              | 200       | 630  | 0.031 秒   | 0.050 秒  | 18 GB        | 64 GB  |
| 8XL      | 1,000,000| 32  | 80              | 200       | 1100 | 0.034 秒   | 0.048 秒  | 19 GB        | 128 GB |
| 12XL     | 1,000,000| 32  | 80              | 200       | 1420 | 0.041 秒   | 0.095 秒  | 21 GB        | 192 GB |
| 16XL     | 1,000,000| 32  | 80              | 200       | 1650 | 0.037 秒   | 0.081 秒  | 23 GB        | 256 GB |

基准测试的准确率为 0.99。

通过增加 [`m` 和 `ef_construction`](/docs/guides/ai/going-to-prod#hnsw-understanding-efconstruction--efsearch--and-m) 也可以提高 QPS。这将允许您为 `ef_search` 使用更小的值并提高 QPS。

</TabPanel>
</Tabs>

### 1536 维 [#hnsw-1536-dimensions]

此基准测试使用 [dbpedia-entities-openai-1M](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) 数据集，其中包含 1,000,000 个文本嵌入。以及针对 `large` 及以下计算附加组件的来