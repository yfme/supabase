---
id: 'ai-concepts'
title: '概念'
description: '了解 AI 和向量应用中的嵌入。'
sidebar_label: '概念'
---

嵌入是许多 AI 和向量应用的核心。本指南涵盖了这些概念。如果您想立即开始，请参阅我们关于[生成嵌入](/docs/guides/ai/quickstarts/generate-text-embeddings)的指南。

## 什么是嵌入？

嵌入捕获文本、图像、视频或其他类型信息的"相关性"。这种相关性最常用于：

- **搜索：**搜索词与文本内容的相似程度如何？
- **推荐：**两个产品的相似程度如何？
- **分类：**如何对文本内容进行分类？
- **聚类：**如何识别趋势？

让我们探索一个文本嵌入的例子。假设我们有三个短语：

1. "猫追逐老鼠"
2. "小猫捕猎啮齿动物"
   {/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}
3. "我喜欢火腿三明治"

您的任务是将具有相似含义的短语分组。如果您是人类，这应该是显而易见的。短语 1 和 2 几乎相同，而短语 3 的含义完全不同。

虽然短语 1 和 2 相似，但它们没有共同的词汇（除了"the"）。然而它们的含义几乎相同。我们如何教计算机这些是相同的？

## 人类语言

人类使用词语和符号来交流语言。但单独的词语大多没有意义——我们需要利用共享知识和经验才能理解它们。"你应该谷歌一下"这句话只有在您知道谷歌是一个搜索引擎，而且人们一直将其用作动词时，才有意义。

同样地，我们需要训练神经网络模型来理解人类语言。一个有效的模型应该在数百万个不同的例子上进行训练，以了解每个单词、短语、句子或段落在不同上下文中可能的含义。

那么这与嵌入有什么关系呢？

## 嵌入如何工作？

嵌入将离散信息（词语和符号）压缩为分布式连续值数据（向量）。如果我们取之前的短语并在图表上绘制，它可能看起来像这样：

<img src="/docs/img/ai/vector-similarity.png" alt="向量相似性" width="640" height="640" />

短语 1 和 2 会被绘制在彼此附近，因为它们的含义相似。我们预计短语 3 会存在于远处的某个地方，因为它不相关。如果我们有第四个短语，"Sally 吃了瑞士奶酪"，这可能存在于短语 3（奶酪可以放在三明治上）和短语 1（老鼠喜欢瑞士奶酪）之间的某个地方。

在这个例子中，我们只有 2 个维度：X 和 Y 轴。在现实中，我们需要更多的维度来有效捕捉人类语言的复杂性。

## 使用嵌入

与上面的二维示例相比，大多数嵌入模型会输出更多维度。例如，开源的 [`gte-small`](https://huggingface.co/Supabase/gte-small) 模型输出 384 个维度。

为什么这很有用？一旦我们对多个文本生成了嵌入，使用向量数学运算（如余弦距离）计算它们的相似度就变得很简单。这种常见用例是搜索。您的流程可能如下所示：

1. 预处理您的知识库并为每个页面生成嵌入
2. 存储您的嵌入以供日后参考
3. 构建一个搜索页面，提示用户输入
4. 获取用户的输入，生成一次性嵌入，然后针对您预处理的嵌入执行相似性搜索
5. 向用户返回最相似的页面

## 另请参阅

- [结构化和非结构化嵌入](/docs/guides/ai/structured-unstructured)
